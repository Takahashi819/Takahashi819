- ğŸ‘‹ Hi, Iâ€™m @Takahashi819
- ğŸ‘€ Iâ€™m interested in ...
- ğŸŒ± Iâ€™m currently learning ...
- ğŸ’ï¸ Iâ€™m looking to collaborate on ...
- ğŸ“« How to reach me ...
- ğŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...

<!---
Takahashi819/Takahashi819 is a âœ¨ special âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
#æ£®æ—ã‚µã‚¤ãƒˆ(XGBoost)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_predict, KFold, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from scipy.stats import gaussian_kde
from sklearn.model_selection import PredefinedSplit

### ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
din_asiaflux = np.loadtxt("/home/test/research/forest.csv", delimiter=",", skiprows=1)
sw = din_asiaflux[:,2]
lai = din_asiaflux[:,5]
ave = din_asiaflux[:,6]
amp = din_asiaflux[:,7]
lst_day = din_asiaflux[:,8]
lst_night = din_asiaflux[:,9]
brdf1 = din_asiaflux[:,10]
brdf2 = din_asiaflux[:,11]
brdf3 = din_asiaflux[:,12]
brdf4 = din_asiaflux[:,13]
brdf5 = din_asiaflux[:,14]
brdf6 = din_asiaflux[:,15]
brdf7 = din_asiaflux[:,16]
lc = din_asiaflux[:,17]
ndvi = (brdf2 - brdf1) / (brdf2 + brdf1)  
evi = 2.5 * (brdf2 - brdf1) / (brdf2 + 6 * brdf1 - 7.5 * brdf3 + 1)
lswi = (brdf2 - brdf6) / (brdf2 + brdf6)
ndwi = (brdf2 - brdf5) / (brdf2 + brdf5)
obs_gpp = din_asiaflux[:,4]

### --- ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã®å®šç¾© ---
feature_combinations = [
    (sw, lst_day, lai, ndvi, lswi, lc),
    (sw, lst_day, lai, ndvi, lswi, ave, amp),
    (sw, lst_day, lai, evi, lswi, lc),
    (sw, lst_day, lai, evi, lswi, ave, amp),
    (sw, lst_day, lai, ndvi, ndwi, lc),
    (sw, lst_day, lai, ndvi, ndwi, ave, amp),
    (sw, lst_day, lai, evi, ndwi, lc),
    (sw, lst_day, lai, evi, ndwi, ave, amp),
    (sw, lst_day, lc, brdf1, brdf2, brdf3, brdf4, brdf5, brdf6, brdf7),
    (sw, lst_day, ave, amp, brdf1, brdf2, brdf3, brdf4, brdf5, brdf6, brdf7)
]

### ã‚«ã‚¹ã‚¿ãƒ åˆ†å‰²ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
split_dict = {
    1: [(2379, 2516), (6410, 6896), (7838, 8007)],
 2: [(1519, 1991), (2517, 2653), (6897, 6987), (7746, 7837)],
 3: [(1992, 2082), (2974, 3018), (4453, 4863), (6988, 7065)],
 4: [(2083, 2195), (2654, 2973), (4864, 5229), (7066, 7202), (8008, 8272)],
 5: [(0, 11), (2196, 2378), (7426, 7745)],
 6: [(12, 142), (3019, 3467), (5632, 5952), (7294, 7425)],
 7: [(143, 305), (3468, 3857), (5953, 6226)],
 8: [(306, 428), (1345, 1518), (5230, 5631)],
 9: [(499, 1344), (6227, 6409)],
 10: [(429, 498), (3858, 4452), (7203, 7293)]
}

### ãƒ‡ãƒ¼ã‚¿é•·ã‚’å–å¾—
num_samples = len(obs_gpp)
custom_fold_indices = np.full(num_samples, -1)

for fold, ranges in split_dict.items():
    for start, end in ranges:
        custom_fold_indices[start:end + 1] = fold - 1

custom_split = PredefinedSplit(custom_fold_indices)

param_grid = {
    'n_estimators': [500, 800, 1200],  
    'max_depth': [5, 8, 12],  
    'learning_rate': [0.005, 0.01, 0.1],  
    'subsample': [0.8],  
    'colsample_bytree': [0.7],  
    'reg_alpha': [0.5, 1],  
    'reg_lambda': [1.0, 2.0],  
    'gamma': [0.5, 2, 5]  
}

best_models = []
best_params_per_fold = {}

### ç‰¹å¾´é‡ã®1ã¤ç›®ã§ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
X = np.column_stack(feature_combinations[0])
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

### å„foldã”ã¨ã«å€‹åˆ¥ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–
for fold in range(np.max(custom_fold_indices) + 1):  # fold ã¯ 0-indexed
    print(f"Processing fold {fold + 1}...")

    train_idx = np.where((custom_fold_indices != fold) & (custom_fold_indices >= 0))[0]
    valid_idx = np.where(custom_fold_indices == fold)[0]

    if valid_idx.size == 0:
        print(f"Warning: No validation data for fold {fold + 1}")
        continue

    X_train, X_valid = X_normalized[train_idx], X_normalized[valid_idx]
    y_train, y_valid = obs_gpp[train_idx], obs_gpp[valid_idx]

    # `train_idx` ã«å¯¾å¿œã—ãŸ `PredefinedSplit`
    fold_split = PredefinedSplit(custom_fold_indices[train_idx])

    # XGB ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    xgb = XGBRegressor()

    # GridSearchCV 
    grid_search = GridSearchCV(xgb, param_grid, scoring='neg_mean_squared_error', cv=fold_split, verbose=1, n_jobs=-1)
    grid_search.fit(X_train, y_train)

    # æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    best_model = grid_search.best_estimator_
    best_models.append(best_model)

    # æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
    best_params_per_fold[fold] = grid_search.best_params_
    print(f"Best Parameters for fold {fold + 1}: {grid_search.best_params_}")

### --- å„foldã®æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡¨ç¤º ---
print("Best Parameters for each fold:")
for fold, params in best_params_per_fold.items():
    print(f"Fold {fold + 1}: {params}")

### --- ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ ---
ensemble_predictions = []
for fold in range(len(best_models)):
    y_pred = best_models[fold].predict(X_normalized)
    ensemble_predictions.append(y_pred)

### --- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¹³å‡ã®è¨ˆç®— ---
ensemble_predictions = np.array(ensemble_predictions)
ensemble_mean = np.mean(ensemble_predictions, axis=0)

print("Cross-validation with per-fold parameter optimization is completed.")

### ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨äºˆæ¸¬
reg_model = LinearRegression()
reg_model.fit(obs_gpp.reshape(-1, 1), ensemble_mean)
predicted_line = reg_model.predict(obs_gpp.reshape(-1, 1))

### æ¿ƒåº¦è¨ˆç®—
xy = np.vstack([obs_gpp, ensemble_mean])
z = gaussian_kde(xy)(xy)

### MSE, RMSE, MBE ã®å†è¨ˆç®—
r_correlation = np.corrcoef(obs_gpp, ensemble_mean)[0, 1]  
r2_regression = r_correlation ** 2  
mse_regression = mean_squared_error(obs_gpp, ensemble_mean)
rmse_regression = np.sqrt(mse_regression)
mbe_regression = np.mean(ensemble_mean - obs_gpp)
data_count = len(obs_gpp)

plt.figure(figsize=(6, 6))
plt.scatter(obs_gpp, ensemble_mean, alpha=0.6, color='blue', s=10)
plt.scatter(obs_gpp, ensemble_mean, c=z, cmap='jet', alpha=0.6, s=10)
plt.plot(obs_gpp, predicted_line, color='red', linewidth=2,
         label=f"forest\n"
               f"data={data_count}\n"
               f"RÂ²={r2_regression:.2g} \n"
               f"RMSE={rmse_regression:.2g}\n"
               f"MBE={mbe_regression:.2g}\n"
               f"y = {reg_model.coef_[0]:.2g}x + {reg_model.intercept_:.2g}")

plt.legend(fontsize=16, loc='upper right')
plt.xlabel(r"Obs GPP(gC m$^{-2}$ day$^{-1}$)", fontsize=20)
plt.ylabel("Model GPP(gC m$^{-2}$ day$^{-1}$)", fontsize=20)
plt.text(1, 26, "XGBoost", fontsize=24, ha='left', va='top', color='black')
plt.xlim(-2.5, 30)
plt.ylim(-2.5, 30)
plt.gca().set_aspect('equal', adjustable='box')

plt.grid(False)
plt.show()

### --- CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ ---
output_path = "/home/test/predict/forest_xgb_results.csv"

### DataFrameä½œæˆ
results_df = pd.DataFrame({
    "Obs_GPP": obs_gpp,
    "Predicted_GPP": ensemble_mean
})

results_df.to_csv(output_path, index=False)

print(f"äºˆæ¸¬çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
print(f"å›å¸°ç›´ç·šã®å¼: y = {reg_model.coef_[0]:.2g}x + {reg_model.intercept_:.2g}")
print(f"RMSE: {rmse_regression:.2g}")
print(f"RÂ² (ç›¸é–¢ä¿‚æ•°Â²): {r2_regression:.2g}")
print(f"MBE: {mbe_regression:.2g}")
